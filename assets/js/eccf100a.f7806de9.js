"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[744],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>v});var r=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=r.createContext({}),d=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},c=function(e){var t=d(e.components);return r.createElement(l.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=d(n),m=o,v=p["".concat(l,".").concat(m)]||p[m]||u[m]||i;return n?r.createElement(v,a(a({ref:t},c),{},{components:n})):r.createElement(v,a({ref:t},c))}));function v(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,a=new Array(i);a[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:o,a[1]=s;for(var d=2;d<i;d++)a[d]=n[d];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},44362:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var r=n(87462),o=(n(67294),n(3905));const i={},a="Search within videos with text",s={unversionedId:"use_cases/items/video_search",id:"use_cases/items/video_search",title:"Search within videos with text",description:"Introduction",source:"@site/content/use_cases/items/video_search.md",sourceDirName:"use_cases/items",slug:"/use_cases/items/video_search",permalink:"/docs/use_cases/items/video_search",draft:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/tree/main/docs/content/use_cases/items/video_search.md",tags:[],version:"current",frontMatter:{},sidebar:"useCasesSidebar",previous:{title:"Vector-search with SuperDuperDB",permalink:"/docs/use_cases/items/vector_search"},next:{title:"Cataloguing voice-memos for a self managed personal assistant",permalink:"/docs/use_cases/items/voice_memos"}},l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Connect to datastore",id:"connect-to-datastore",level:2},{value:"Load Dataset",id:"load-dataset",level:2},{value:"Register Encoders",id:"register-encoders",level:2},{value:"Create CLIP model",id:"create-clip-model",level:2},{value:"Create VectorIndex",id:"create-vectorindex",level:2},{value:"Query a text against saved frames.",id:"query-a-text-against-saved-frames",level:2},{value:"Start the video from the resultant timestamp:",id:"start-the-video-from-the-resultant-timestamp",level:2}],c={toc:d},p="wrapper";function u(e){let{components:t,...n}=e;return(0,o.kt)(p,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"search-within-videos-with-text"},"Search within videos with text"),(0,o.kt)("h2",{id:"introduction"},"Introduction"),(0,o.kt)("p",null,"This notebook outlines the process of searching for specific textual information within videos and retrieving relevant video segments. To accomplish this, we utilize various libraries and techniques, such as:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"clip: A library for vision and language understanding."),(0,o.kt)("li",{parentName:"ul"},"PIL: Python Imaging Library for image processing."),(0,o.kt)("li",{parentName:"ul"},"torch: The PyTorch library for deep learning.")),(0,o.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,o.kt)("p",null,"Before diving into the implementation, ensure that you have the necessary libraries installed by running the following commands:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"!pip install superduperdb\n!pip install ipython opencv-python pillow openai-clip\n")),(0,o.kt)("h2",{id:"connect-to-datastore"},"Connect to datastore"),(0,o.kt)("p",null,"First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the ",(0,o.kt)("inlineCode",{parentName:"p"},"MongoDB_URI")," based on your specific setup.\nHere are some examples of MongoDB URIs:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"For testing (default connection): ",(0,o.kt)("inlineCode",{parentName:"li"},"mongomock://test")),(0,o.kt)("li",{parentName:"ul"},"Local MongoDB instance: ",(0,o.kt)("inlineCode",{parentName:"li"},"mongodb://localhost:27017")),(0,o.kt)("li",{parentName:"ul"},"MongoDB with authentication: ",(0,o.kt)("inlineCode",{parentName:"li"},"mongodb://superduper:superduper@mongodb:27017/documents")),(0,o.kt)("li",{parentName:"ul"},"MongoDB Atlas: ",(0,o.kt)("inlineCode",{parentName:"li"},"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb import superduper\nfrom superduperdb.backends.mongodb import Collection\nfrom superduperdb import CFG\nimport os\n\nCFG.downloads.hybrid = True\nCFG.downloads.root = './'\n\nmongodb_uri = os.getenv(\"MONGODB_URI\",\"mongomock://test\")\ndb = superduper(mongodb_uri, artifact_store='filesystem://./data/')\n\nvideo_collection = Collection('videos')\n")),(0,o.kt)("h2",{id:"load-dataset"},"Load Dataset"),(0,o.kt)("p",null,"We'll begin by configuring a video encoder."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb import Encoder\n\nvid_enc = Encoder(\n    identifier='video_on_file',\n    load_hybrid=False,\n)\n\ndb.add(vid_enc)\n")),(0,o.kt)("p",null,"Now, let's retrieve a sample video from the internet and insert it into our collection."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb.base.document import Document\n\ndb.execute(video_collection.insert_one(\n        Document({'video': vid_enc(uri='https://superduperdb-public.s3.eu-west-1.amazonaws.com/animals_excerpt.mp4')})\n    )\n)\n\n# Display the list of videos in the collection\nlist(db.execute(Collection('videos').find()))\n")),(0,o.kt)("h2",{id:"register-encoders"},"Register Encoders"),(0,o.kt)("p",null,"Next, we'll create encoders for processing videos and extracting frames. This encoder will help us convert videos into individual frames."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import cv2\nimport tqdm\nfrom PIL import Image\nfrom superduperdb.ext.pillow import pil_image\nfrom superduperdb import Model, Schema\n\n\ndef video2images(video_file):\n    sample_freq = 10\n    cap = cv2.VideoCapture(video_file)\n\n    frame_count = 0\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    print(fps)\n    extracted_frames = []\n    progress = tqdm.tqdm()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        current_timestamp = frame_count // fps\n        \n        if frame_count % sample_freq == 0:\n            extracted_frames.append({\n                'image': Image.fromarray(frame[:,:,::-1]),\n                'current_timestamp': current_timestamp,\n            })\n        frame_count += 1        \n        progress.update(1)\n    \n    cap.release()\n    cv2.destroyAllWindows()\n    return extracted_frames\n\n\nvideo2images = Model(\n    identifier='video2images',\n    object=video2images,\n    flatten=True,\n    model_update_kwargs={'document_embedded': False},\n    output_schema=Schema(identifier='myschema', fields={'image': pil_image})\n)\n")),(0,o.kt)("p",null,"We'll also set up a listener to continuously download video URLs and save the best frames into another collection."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb import Listener\n\ndb.add(\n   Listener(\n       model=video2images,\n       select=video_collection.find(),\n       key='video',\n   )\n)\n\ndb.execute(Collection('_outputs.video.video2images').find_one()).unpack()['_outputs']['video']['video2images']['image']\n")),(0,o.kt)("h2",{id:"create-clip-model"},"Create CLIP model"),(0,o.kt)("p",null,"Now, we'll create a model for the CLIP (Contrastive Language-Image Pre-training) model, which will be used for visual and textual analysis."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import clip\nfrom superduperdb import vector\nfrom superduperdb.ext.torch import TorchModel\n\nmodel, preprocess = clip.load(\"RN50\", device='cpu')\nt = vector(shape=(1024,))\n\nvisual_model = TorchModel(\n    identifier='clip_image',\n    preprocess=preprocess,\n    object=model.visual,\n    encoder=t,\n    postprocess=lambda x: x.tolist(),\n)\n\ntext_model = TorchModel(\n    identifier='clip_text',\n    object=model,\n    preprocess=lambda x: clip.tokenize(x)[0],\n    forward_method='encode_text',\n    encoder=t,\n    device='cpu',\n    preferred_devices=None,\n    postprocess=lambda x: x.tolist(),\n)\n")),(0,o.kt)("h2",{id:"create-vectorindex"},"Create VectorIndex"),(0,o.kt)("p",null,"We will set up a VectorIndex to index and search the video frames based on both visual and textual content. This involves creating an indexing listener for visual data and a compatible listener for textual data."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb import Listener, VectorIndex\nfrom superduperdb.backends.mongodb import Collection\n\ndb.add(\n    VectorIndex(\n        identifier='video_search_index',\n        indexing_listener=Listener(\n            model=visual_model,\n            key='_outputs.video.video2images.image',\n            select=Collection('_outputs.video.video2images').find(),\n        ),\n        compatible_listener=Listener(\n            model=text_model,\n            key='text',\n            select=None,\n            active=False\n        )\n    )\n)\n")),(0,o.kt)("h2",{id:"query-a-text-against-saved-frames"},"Query a text against saved frames."),(0,o.kt)("p",null,"Now, let's search for something that happened during the video:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Define the search parameters\nsearch_term = 'Some ducks'\nnum_results = 1\n\n\nr = next(db.execute(\n    Collection('_outputs.video.video2images').like(Document({'text': search_term}), vector_index='video_search_index', n=num_results).find()\n))\n\nsearch_timestamp = r['_outputs']['video']['video2images']['current_timestamp']\n\n# Get the back reference to the original video\nvideo = db.execute(Collection('videos').find_one({'_id': r['_source']}))\n")),(0,o.kt)("h2",{id:"start-the-video-from-the-resultant-timestamp"},"Start the video from the resultant timestamp:"),(0,o.kt)("p",null,"Finally, we can display and play the video starting from the timestamp where the searched text is found."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from IPython.display import display, HTML\n\nvideo_html = f"""\n<video width="640" height="480" controls>\n    <source src="{video[\'video\'].uri}" type="video/mp4">\n</video>\n<script>\n    var video = document.querySelector(\'video\');\n    video.currentTime = {search_timestamp};\n    video.play();\n<\/script>\n"""\n\ndisplay(HTML(video_html))\n')))}u.isMDXComponent=!0}}]);