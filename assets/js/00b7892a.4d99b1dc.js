"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[524],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>f});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),d=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=d(e.components);return r.createElement(l.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=d(n),m=a,f=p["".concat(l,".").concat(m)]||p[m]||u[m]||o;return n?r.createElement(f,i(i({ref:t},c),{},{components:n})):r.createElement(f,i({ref:t},c))}));function f(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:a,i[1]=s;for(var d=2;d<o;d++)i[d]=n[d];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},95358:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var r=n(87462),a=(n(67294),n(3905));const o={},i="Transfer Learning with Sentence Transformers and Scikit-Learn",s={unversionedId:"use_cases/items/transfer_learning",id:"use_cases/items/transfer_learning",title:"Transfer Learning with Sentence Transformers and Scikit-Learn",description:"Introduction",source:"@site/content/use_cases/items/transfer_learning.md",sourceDirName:"use_cases/items",slug:"/use_cases/items/transfer_learning",permalink:"/docs/use_cases/items/transfer_learning",draft:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/tree/main/docs/content/use_cases/items/transfer_learning.md",tags:[],version:"current",frontMatter:{},sidebar:"useCasesSidebar",previous:{title:"End-2-end example using SQL databases",permalink:"/docs/use_cases/items/sql-example"},next:{title:"Vector-search with SuperDuperDB",permalink:"/docs/use_cases/items/vector_search"}},l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Connect to datastore",id:"connect-to-datastore",level:2},{value:"Load Dataset",id:"load-dataset",level:2},{value:"Run Model",id:"run-model",level:2},{value:"Train Downstream Model",id:"train-downstream-model",level:2},{value:"Run Downstream Model",id:"run-downstream-model",level:2},{value:"Verification",id:"verification",level:2}],c={toc:d},p="wrapper";function u(e){let{components:t,...n}=e;return(0,a.kt)(p,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"transfer-learning-with-sentence-transformers-and-scikit-learn"},"Transfer Learning with Sentence Transformers and Scikit-Learn"),(0,a.kt)("h2",{id:"introduction"},"Introduction"),(0,a.kt)("p",null,"In this notebook, we will explore the process of transfer learning using SuperDuperDB. We will demonstrate how to connect to a MongoDB datastore, load a dataset, create a SuperDuperDB model based on Sentence Transformers, train a downstream model using Scikit-Learn, and apply the trained model to the database. Transfer learning is a powerful technique that can be used in various applications, such as vector search and downstream learning tasks."),(0,a.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,a.kt)("p",null,"Before diving into the implementation, ensure that you have the necessary libraries installed by running the following commands:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"!pip install superduperdb\n!pip install ipython numpy datasets sentence-transformers\n")),(0,a.kt)("h2",{id:"connect-to-datastore"},"Connect to datastore"),(0,a.kt)("p",null,"First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the ",(0,a.kt)("inlineCode",{parentName:"p"},"MongoDB_URI")," based on your specific setup.\nHere are some examples of MongoDB URIs:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"For testing (default connection): ",(0,a.kt)("inlineCode",{parentName:"li"},"mongomock://test")),(0,a.kt)("li",{parentName:"ul"},"Local MongoDB instance: ",(0,a.kt)("inlineCode",{parentName:"li"},"mongodb://localhost:27017")),(0,a.kt)("li",{parentName:"ul"},"MongoDB with authentication: ",(0,a.kt)("inlineCode",{parentName:"li"},"mongodb://superduper:superduper@mongodb:27017/documents")),(0,a.kt)("li",{parentName:"ul"},"MongoDB Atlas: ",(0,a.kt)("inlineCode",{parentName:"li"},"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from superduperdb import superduper\nfrom superduperdb.backends.mongodb import Collection\nimport os\n\nmongodb_uri = os.getenv("MONGODB_URI","mongomock://test")\ndb = superduper(mongodb_uri)\n\ncollection = Collection(\'transfer\')\n')),(0,a.kt)("h2",{id:"load-dataset"},"Load Dataset"),(0,a.kt)("p",null,"Transfer learning can be applied to any data that can be processed with SuperDuperDB models.\nFor our example, we will use a labeled textual dataset with sentiment analysis.  We'll load a subset of the IMDb dataset."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import numpy\nfrom datasets import load_dataset\nfrom superduperdb import Document as D\n\ndata = load_dataset(\"imdb\")\n\nN_DATAPOINTS = 500    # Increase for higher quality\n\ntrain_data = [\n    D({'_fold': 'train', **data['train'][int(i)]}) \n    for i in numpy.random.permutation(len(data['train']))\n][:N_DATAPOINTS]\n\nvalid_data = [\n    D({'_fold': 'valid', **data['test'][int(i)]}) \n    for i in numpy.random.permutation(len(data['test']))\n][:N_DATAPOINTS // 10]\n\ndb.execute(collection.insert_many(train_data))\n")),(0,a.kt)("h2",{id:"run-model"},"Run Model"),(0,a.kt)("p",null,"We'll create a SuperDuperDB model based on the ",(0,a.kt)("inlineCode",{parentName:"p"},"sentence_transformers")," library. This demonstrates that you don't necessarily need a native SuperDuperDB integration with a model library to leverage its power. We configure the ",(0,a.kt)("inlineCode",{parentName:"p"},"Model wrapper")," to work with the ",(0,a.kt)("inlineCode",{parentName:"p"},"SentenceTransformer class"),". After configuration, we can link the model to a collection and daemonize the model with the ",(0,a.kt)("inlineCode",{parentName:"p"},"listen=True")," keyword."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb import Model\nimport sentence_transformers\nfrom superduperdb.ext.numpy import array\n\nm = Model(\n    identifier='all-MiniLM-L6-v2',\n    object=sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2'),\n    encoder=array('float32', shape=(384,)),\n    predict_method='encode',\n    batch_predict=True,\n)\n\nm.predict(\n    X='text',\n    db=db,\n    select=collection.find(),\n    listen=True\n)\n")),(0,a.kt)("h2",{id:"train-downstream-model"},"Train Downstream Model"),(0,a.kt)("p",null,"Now that we've created and added the model that computes features for the ",(0,a.kt)("inlineCode",{parentName:"p"},'"text"'),", we can train a downstream model using Scikit-Learn."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.svm import SVC\n\nmodel = superduper(\n    SVC(gamma='scale', class_weight='balanced', C=100, verbose=True),\n    postprocess=lambda x: int(x)\n)\n\nmodel.fit(\n    X='text',\n    y='label',\n    db=db,\n    select=collection.find().featurize({'text': 'all-MiniLM-L6-v2'}),\n)\n")),(0,a.kt)("h2",{id:"run-downstream-model"},"Run Downstream Model"),(0,a.kt)("p",null,"With the model trained, we can now apply it to the database. "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"model.predict(\n    X='text',\n    db=db,\n    select=collection.find().featurize({'text': 'all-MiniLM-L6-v2'}),\n    listen=True,\n)\n")),(0,a.kt)("h2",{id:"verification"},"Verification"),(0,a.kt)("p",null,"To verify that the process has worked, we can sample a few records to inspect the sanity of the predictions."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"r = next(db.execute(collection.aggregate([{'$sample': {'size': 1}}])))\nprint(r['text'][:100])\nprint(r['_outputs']['text']['svc'])\n")))}u.isMDXComponent=!0}}]);