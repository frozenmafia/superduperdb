"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[8569],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>h});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(n),u=r,h=d["".concat(l,".").concat(u)]||d[u]||m[u]||o;return n?a.createElement(h,i(i({ref:t},p),{},{components:n})):a.createElement(h,i({ref:t},p))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[d]="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=n[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},3431:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var a=n(87462),r=(n(67294),n(3905));const o={},i="Multimodal Search Using CLIP",s={unversionedId:"use_cases/items/multimodal_image_search_clip",id:"use_cases/items/multimodal_image_search_clip",title:"Multimodal Search Using CLIP",description:"Introduction",source:"@site/content/use_cases/items/multimodal_image_search_clip.md",sourceDirName:"use_cases/items",slug:"/use_cases/items/multimodal_image_search_clip",permalink:"/docs/use_cases/items/multimodal_image_search_clip",draft:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/tree/main/docs/content/use_cases/items/multimodal_image_search_clip.md",tags:[],version:"current",frontMatter:{},sidebar:"useCasesSidebar",previous:{title:"Training and Maintaining MNIST Predictions with SuperDuperDB",permalink:"/docs/use_cases/items/mnist_torch"},next:{title:"Building Q&A Assistant Using Mongo and OpenAI",permalink:"/docs/use_cases/items/question_the_docs"}},l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Connect to datastore",id:"connect-to-datastore",level:2},{value:"Load Dataset",id:"load-dataset",level:2},{value:"Build Models",id:"build-models",level:2},{value:"Create a Vector-Search Index",id:"create-a-vector-search-index",level:2},{value:"Search Images Using Text",id:"search-images-using-text",level:2}],p={toc:c},d="wrapper";function m(e){let{components:t,...n}=e;return(0,r.kt)(d,(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"multimodal-search-using-clip"},"Multimodal Search Using CLIP"),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"This notebook showcases the capabilities of SuperDuperDB for performing multimodal searches using the ",(0,r.kt)("inlineCode",{parentName:"p"},"VectorIndex"),". SuperDuperDB's flexibility enables users and developers to integrate various models into the system and use them for vectorizing diverse queries during search and inference. In this demonstration, we leverage the ",(0,r.kt)("a",{parentName:"p",href:"https://openai.com/research/clip"},"CLIP multimodal architecture"),"."),(0,r.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,r.kt)("p",null,"Before diving into the implementation, ensure that you have the necessary libraries installed by running the following commands:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"!pip install superduperdb\n!pip install ipython openai-clip\n!pip install -U datasets\n")),(0,r.kt)("h2",{id:"connect-to-datastore"},"Connect to datastore"),(0,r.kt)("p",null,"First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"MongoDB_URI")," based on your specific setup.\nHere are some examples of MongoDB URIs:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"For testing (default connection): ",(0,r.kt)("inlineCode",{parentName:"li"},"mongomock://test")),(0,r.kt)("li",{parentName:"ul"},"Local MongoDB instance: ",(0,r.kt)("inlineCode",{parentName:"li"},"mongodb://localhost:27017")),(0,r.kt)("li",{parentName:"ul"},"MongoDB with authentication: ",(0,r.kt)("inlineCode",{parentName:"li"},"mongodb://superduper:superduper@mongodb:27017/documents")),(0,r.kt)("li",{parentName:"ul"},"MongoDB Atlas: ",(0,r.kt)("inlineCode",{parentName:"li"},"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import os\nfrom superduperdb import superduper\nfrom superduperdb.backends.mongodb import Collection\n\nmongodb_uri = os.getenv(\"MONGODB_URI\", \"mongomock://test\")\ndb = superduper(mongodb_uri, artifact_store='filesystem://./models/')\n\n# Super-Duper your Database!\ndb = superduper(mongodb_uri, artifact_store='filesystem://.data')\n\ncollection = Collection('multimodal')\n")),(0,r.kt)("h2",{id:"load-dataset"},"Load Dataset"),(0,r.kt)("p",null,"To make this notebook easily executable and interactive, we'll work with a sub-sample of the ",(0,r.kt)("a",{parentName:"p",href:"https://paperswithcode.com/dataset/tiny-imagenet"},"Tiny-Imagenet dataset"),". The processes demonstrated here can be applied to larger datasets with higher resolution images as well. For such use-cases, however, it's advisable to use a machine with a GPU, otherwise they'll be some significant thumb twiddling to do."),(0,r.kt)("p",null,"To insert images into the database, we utilize the ",(0,r.kt)("inlineCode",{parentName:"p"},"Encoder"),"-",(0,r.kt)("inlineCode",{parentName:"p"},"Document")," framework, which allows saving Python class instances as blobs in the ",(0,r.kt)("inlineCode",{parentName:"p"},"Datalayer")," and retrieving them as Python objects. To this end, SuperDuperDB contains pre-configured support for ",(0,r.kt)("inlineCode",{parentName:"p"},"PIL.Image")," instances. This simplifies the integration of Python AI models with the datalayer. It's also possible to create your own encoders."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"!curl -O https://superduperdb-public.s3.eu-west-1.amazonaws.com/coco_sample.zip\n!unzip coco_sample.zip\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb import Document\nfrom superduperdb.ext.pillow import pil_image as i\nimport glob\nimport random\n\nimages = glob.glob('images_small/*.jpg')\ndocuments = [Document({'image': i(uri=f'file://{img}')}) for img in images][:500]\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"documents[1]\n")),(0,r.kt)("p",null,"The wrapped python dictionaries may be inserted directly to the ",(0,r.kt)("inlineCode",{parentName:"p"},"Datalayer"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"db.execute(collection.insert_many(documents), encoders=(i,))\n")),(0,r.kt)("p",null,"You can verify that the images are correctly stored as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"x = db.execute(imagenet_collection.find_one()).unpack()['image']\ndisplay(x.resize((300, 300 * int(x.size[1] / x.size[0]))))\n")),(0,r.kt)("h2",{id:"build-models"},"Build Models"),(0,r.kt)("p",null,"We now can wrap the CLIP model, to ready it for multimodal search. It involves 2 components:"),(0,r.kt)("p",null,"Now, let's prepare the CLIP model for multimodal search, which involves two components: ",(0,r.kt)("inlineCode",{parentName:"p"},"text encoding")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"visual encoding"),". After installing both components, you can perform searches using both images and text to find matching items:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import clip\nfrom superduperdb import vector\nfrom superduperdb.ext.torch import TorchModel\n\n# Load the CLIP model\nmodel, preprocess = clip.load(\"RN50\", device='cpu')\n\n# Define a vector\ne = vector(shape=(1024,))\n\n# Create a TorchModel for text encoding\ntext_model = TorchModel(\n    identifier='clip_text',\n    object=model,\n    preprocess=lambda x: clip.tokenize(x)[0],\n    postprocess=lambda x: x.tolist(),\n    encoder=e,\n    forward_method='encode_text',    \n)\n\n# Create a TorchModel for visual encoding\nvisual_model = TorchModel(\n    identifier='clip_image',\n    object=model.visual,    \n    preprocess=preprocess,\n    postprocess=lambda x: x.tolist(),\n    encoder=e,\n)\n")),(0,r.kt)("h2",{id:"create-a-vector-search-index"},"Create a Vector-Search Index"),(0,r.kt)("p",null,"Let's create the index for vector-based searching. We'll register both models with the index simultaneously, but specify that the ",(0,r.kt)("inlineCode",{parentName:"p"},"visual_model")," will be responsible for creating the vectors in the database (",(0,r.kt)("inlineCode",{parentName:"p"},"indexing_listener"),"). The ",(0,r.kt)("inlineCode",{parentName:"p"},"compatible_listener")," specifies how an alternative model can be used to search the vectors, enabling multimodal search with models expecting different types of indexes."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb import VectorIndex\nfrom superduperdb import Listener\n\n# Create a VectorIndex and add it to the database\ndb.add(\n    VectorIndex(\n        'my-index',\n        indexing_listener=Listener(\n            model=visual_model,\n            key='image',\n            select=collection.find(),\n            predict_kwargs={'batch_size': 10},\n        ),\n        compatible_listener=Listener(\n            model=text_model,\n            key='text',\n            active=False,\n            select=None,\n        )\n    )\n)\n")),(0,r.kt)("h2",{id:"search-images-using-text"},"Search Images Using Text"),(0,r.kt)("p",null,"Now we can demonstrate searching for images using text queries:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from IPython.display import display\nfrom superduperdb import Document\n\nquery_string = 'sports'\n\nout = db.execute(\n    collection.like(Document({'text': query_string}), vector_index='my-index', n=3).find({})\n)\n\n# Display the images from the search results\nfor r in search_results:\n    x = r['image'].x\n    display(x.resize((300, int(300 * x.size[1] / x.size[0]))))\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"img = db.execute(collection.find_one({}))['image']\nimg.x\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"cur = db.execute(\n    collection.like(Document({'image': img}), vector_index='my-index', n=3).find({})\n)\n\nfor r in cur:\n    x = r['image'].x\n    display(x.resize((300, int(300 * x.size[1] / x.size[0]))))\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"")))}m.isMDXComponent=!0}}]);